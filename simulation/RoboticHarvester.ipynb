{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34921a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal as torchNormal\n",
    "import gym\n",
    "\n",
    "# Custom env\n",
    "from CustomEnv import Env as ENV\n",
    "\n",
    "# Libraries for the RL training\n",
    "from utils.NetModels import Actor, Critic, Agent, DummyActor, ImmobileActor\n",
    "from utils.AuxFuncs import (save_net, load_net, plot_stats)\n",
    "from utils.TrainingFuncs import train_agent\n",
    "from utils.RlAlgorithms import SAC\n",
    "\n",
    "# Libraries for the OSC\n",
    "from abr_control.interfaces.mujoco import Mujoco as MujocoInterface\n",
    "from abr_control.arms.mujoco_config import MujocoConfig as ArmConfig\n",
    "from abr_control.controllers.osc import OSC\n",
    "from abr_control.controllers.joint_limit_controller import JointLimitsController\n",
    "from abr_control.controllers.hierarchy_controller import HierarchyController\n",
    "\n",
    "# Auxiliar libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glfw\n",
    "from collections import deque\n",
    "from random import sample\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4d2362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enviroment Instantiation ---\n",
    "\n",
    "xml_model = \"franka_research3.xml\"    \n",
    "#folder_path = \"c:\\\\Users\\\\user\\\\Desktop\\\\folder\\\\assets\"\n",
    "\n",
    "DOF = 7\n",
    "FINGER_DOF = 2\n",
    "initial_pose = np.array([0, -np.pi/4, 0, -3*np.pi/4, 0, np.pi/2, np.pi/4])\n",
    "\n",
    "env = ENV(folder_path + \"\\\\\" + xml_model, DOF = DOF,\n",
    "                   FINGER_DOF = FINGER_DOF, initial_pose = initial_pose)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7b62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControllerWrapper():\n",
    "\n",
    "\n",
    "    def __init__(self, dist_thresh1, angle_thresh,\n",
    "                 grab_dist_thresh, aux_control_bool, DOF, FINGER_DOF):\n",
    "        self.null_control = None\n",
    "        self.dist_thresh1 = dist_thresh1\n",
    "        self.angle_thresh = angle_thresh\n",
    "        self.grab_dist_thresh = grab_thresh\n",
    "        self.grab_finger_thresh = 0.01\n",
    "        self.DOF = DOF \n",
    "        self.FINGER_DOF = FINGER_DOF\n",
    "    \n",
    "        self.fruit_grabbed = False\n",
    "        self.aux_control_bool = aux_control_bool\n",
    "        self.aux_control_active = False\n",
    "        \n",
    "        self.custom_target = None\n",
    "        self.agent_null_torque = np.array([0]*self.DOF)\n",
    "        \n",
    "    def pre_control_func(self, agent_target, agent_null_torque, env_dict):\n",
    "        \n",
    "        if np.any(self.custom_target):\n",
    "            target = self.custom_target.copy()\n",
    "            self.custom_target = None\n",
    "            \n",
    "        else:\n",
    "            target = env.get_body_com(\"target\")\n",
    "            \n",
    "        target_dist = np.linalg.norm(target - env.get_body_com(\"EE\"))\n",
    "        target_pos = env._dict[\"target_pos\"]\n",
    "        target_angle = env._dict[\"target_angle\"]\n",
    "        finger_angles = env._dict[\"finger_angles\"] \n",
    "        self.agent_null_torque = agent_null_torque\n",
    "    \n",
    "        if self.aux_control_active:\n",
    "            agent_target = target_pos\n",
    "        else:\n",
    "            if self.aux_control_bool:\n",
    "                if target_dist < self.dist_thresh1 and target_angle <= self.angle_thresh:\n",
    "                    self.aux_control_active = True\n",
    "                    agent_target = target_pos\n",
    "                    \n",
    "        finger_act = None\n",
    "\n",
    "        if target_dist < self.grab_dist_thresh:\n",
    "\n",
    "            finger_act = np.array([1] * self.FINGER_DOF) * -5\n",
    "            fing_closed_bool = abs(finger_angles) < self.grab_finger_thresh \n",
    "            if np.any(fing_closed_bool):\n",
    "                self.fruit_grabbed = True\n",
    "                self.aux_control_active = True\n",
    "        else:\n",
    "            if self.fruit_grabbed:\n",
    "                finger_act = np.array([1] * self.FINGER_DOF) * -7\n",
    "        finger_act = None\n",
    "        return agent_target, finger_act\n",
    "            \n",
    "    def generate(self, q, dq, target_velocity=None):\n",
    "        if not self.aux_control_active:\n",
    "            agent_null_torque = self.agent_null_torque\n",
    "        else:\n",
    "            agent_null_torque = np.array([0]*self.DOF)\n",
    "            \n",
    "        return agent_null_torque\n",
    "    \n",
    "    def reset(self, custom_target = None):\n",
    "        self.aux_control_active = False\n",
    "        self.fruit_grabbed = False\n",
    "        \n",
    "        self.agent_null_torque = np.array([0]*self.DOF)\n",
    "        \n",
    "        if np.any(custom_target):\n",
    "            self.custom_target = custom_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b571b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Controller gains ---\n",
    "dt = 0.001\n",
    "kp = 1 \n",
    "kv = 2\n",
    "ki = 0.0\n",
    "\n",
    "# --- Auxiliar controller parameters---\n",
    "dist_thresh1 = 0.15 \n",
    "angle_thresh = 60 * (np.pi/180)\n",
    "grab_thresh = 0.03\n",
    "aux_control_bool = True\n",
    "torque_safety_factor = 0.9 # porcentaje del torque maximo que se utiliza\n",
    "min_torque = env.action_space.low * torque_safety_factor\n",
    "max_torque = env.action_space.high * torque_safety_factor\n",
    "\n",
    "# --- Parametros del controlador de seguridad ---\n",
    "\n",
    "danger_perc = 0.95 \n",
    "warning_perc = 0.9 \n",
    "env.safe_perc = 0.5 \n",
    "\n",
    "joint_mid_range = (env.q_limit_high + env.q_limit_low)/2\n",
    "joint_range = ((env.q_limit_high - env.q_limit_low)/2) * danger_perc\n",
    "min_joint_angles = joint_mid_range - joint_range\n",
    "max_joint_angles = joint_mid_range + joint_range\n",
    "\n",
    "joint_angles_thresh = joint_range *  (1 - warning_perc)  \n",
    "kp_safe = env.action_space.high * 1 \n",
    "kd_safe = env.action_space.high * 0 \n",
    "\n",
    "ctrlr_dof=[True, True, True, False, False, False] \n",
    "vmax = None \n",
    "\n",
    "robot_config = ArmConfig(xml_model, folder = folder_path)\n",
    "interface = MujocoInterface(robot_config, dt=dt, visualize = False)\n",
    "env.sim.model.opt.timestep = dt\n",
    "interface.connect(sim = env.sim) \n",
    "\n",
    "\n",
    "null_controller = ControllerWrapper(dist_thresh1, angle_thresh,\n",
    "                 grab_thresh, aux_control_bool, DOF, FINGER_DOF)\n",
    "\n",
    "op_space_controller = OSC2(robot_config, kp = kp, kv = kv, ki = ki, vmax = vmax)\n",
    "\n",
    "safety_controller = JointLimitsController(robot_config = robot_config, min_joint_angles = min_joint_angles,\n",
    "                      max_joint_angles = max_joint_angles, joint_angles_thresh = joint_angles_thresh,\n",
    "                      k_p = kp_safe, k_d = kd_safe)\n",
    "\n",
    "controller = HierarchyController(robot_config = robot_config, safe_controller = safety_controller,\n",
    "                                 min_torque = min_torque, max_torque = max_torque,\n",
    "                                 op_space_controller = op_space_controller, null_controllers= [null_controller])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a9e9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neural network architecture definition ---\n",
    "def net_architecture(input_dim, hidden_dim):\n",
    "    return nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                         nn.LayerNorm(hidden_dim),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Linear(hidden_dim, hidden_dim),\n",
    "                         nn.LayerNorm(hidden_dim),\n",
    "                         nn.LeakyReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd2d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definition of the actor/critics networks---\n",
    "\n",
    "action_dim = 10 # x_desired + t_null\n",
    "state_dim = env.observation_space.shape[0]\n",
    "hidden_dim = 500 \n",
    "\n",
    "max_action_range = torch.Tensor(np.concatenate((np.array([0.45] * 3),\n",
    "                                                max_torque * 0.5))).to(device) \n",
    "\n",
    "min_std = 0.01 \n",
    "max_std = 0.4\n",
    "\n",
    "actor = Actor(\"SAC\", net_architecture(state_dim, hidden_dim), hidden_dim, action_dim, max_action_range,\n",
    "              min_std, max_std).to(device)\n",
    " \n",
    "q1_critic = Critic(\"SAC\", net_architecture(state_dim + action_dim, hidden_dim), hidden_dim).to(device)\n",
    "target_q1_critic = Critic(\"SAC\", net_architecture(state_dim + action_dim, hidden_dim), hidden_dim).to(device)\n",
    "\n",
    "q2_critic = Critic(\"SAC\", net_architecture(state_dim + action_dim, hidden_dim), hidden_dim).to(device)\n",
    "target_q2_critic = Critic(\"SAC\", net_architecture(state_dim + action_dim, hidden_dim), hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29696717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentEnv():\n",
    "\n",
    "    \n",
    "    def __init__(self, env, skip_steps, controller, aux_controller, max_steps, max_range,\n",
    "                 ctrl_const, crash_const, sucess_const, time_const,\n",
    "                 danger_perc, warning_perc, aux_control = False):\n",
    "        \n",
    "        self.env = env\n",
    "        self.skip_steps = skip_steps\n",
    "        self.controller = controller\n",
    "        self.aux_controller = aux_controller\n",
    "        self._max_episode_steps = max_steps  \n",
    "        self.env._max_episode_steps = self._max_episode_steps * self.skip_steps\n",
    "        self.max_range = max_range\n",
    "        self.ctrl_const = ctrl_const\n",
    "        self.crash_const = crash_const\n",
    "        self.time_const = time_const\n",
    "        self.final_success_const = sucess_const\n",
    "        self.success_bool = False\n",
    "        self.n_actions = env.action_space.shape[0]\n",
    "        self.q = np.zeros(self.n_actions) * 2 \n",
    "        self.dq = np.zeros(self.n_actions)\n",
    "        self.DOF = env.DOF\n",
    "        \n",
    "        self.render = False\n",
    "        self.danger_perc = danger_perc\n",
    "        self.warning_perc = warning_perc\n",
    "        \n",
    "        self.dist_measure = 0\n",
    "        self.energy_measure = 0\n",
    "        self.manipu_measure = 0\n",
    "        self.finish_time = np.inf\n",
    "        \n",
    "        self.TESTING = False\n",
    "\n",
    "        class Aux():\n",
    "            pass\n",
    "        self.action_space = Aux()\n",
    "        self.action_space.sample = lambda: np.random.uniform(-self.max_range, self.max_range, 3 + self.DOF)\n",
    "\n",
    "    def draw_obj(self, agent_target, agent_null_torque):\n",
    "        \n",
    "        robot_joint_index_ini = self.env.model.geom_name2id('link_1_visual_geom')\n",
    "        robot_joint_indexes = np.arange(robot_joint_index_ini, robot_joint_index_ini + 2 * self.DOF, 2)\n",
    "        \n",
    "        joint_range = (self.env.q_limit_high - self.env.q_limit_low) \n",
    "        high_dist = self.env.q_limit_high * self.danger_perc - self.env.q \n",
    "        low_dist = self.env.q - self.env.q_limit_low * self.danger_perc\n",
    "        min_dist = np.minimum(high_dist, low_dist)\n",
    "\n",
    "        safe_index = min_dist > joint_range/2 * (1 - self.warning_perc)\n",
    "        danger_index = min_dist <= 0\n",
    "        warning_index = ~ safe_index & ~ danger_index\n",
    "        \n",
    "        safe_joints = robot_joint_indexes[safe_index]\n",
    "        warning_joints = robot_joint_indexes[warning_index]\n",
    "        danger_joints = robot_joint_indexes[danger_index]\n",
    "        \n",
    "        self.env.model.geom_rgba[safe_joints] = np.array([0.3, 0.9, 0.2, 0.9])\n",
    "        self.env.model.geom_rgba[warning_joints] = np.array([0.9, 0.9, 0.2, 0.9])\n",
    "        self.env.model.geom_rgba[danger_joints] = np.array([0.9, 0.3, 0.2, 1])\n",
    "        \n",
    "        self.env.data.mocap_pos[2] = agent_target\n",
    "\n",
    "    def step(self, agent_output, stop_early = True):\n",
    "        agent_target = agent_output[0:3]\n",
    "        agent_null_torque = agent_output[3:]\n",
    "        \n",
    "        agent_target = self.env.get_body_com(\"EE\") + agent_target\n",
    "        \n",
    "        if self.reset_step:\n",
    "            finger_act = None\n",
    "            self.reset_step = False\n",
    "        else:\n",
    "            agent_target, finger_act = self.aux_controller.pre_control_func(agent_target,\n",
    "                                                                            agent_null_torque,\n",
    "                                                                            self.env._dict)\n",
    "\n",
    "        if self.aux_controller.aux_control_active and not self.success_bool:\n",
    "            self.success_bool = True\n",
    "\n",
    "        step_reward = 0\n",
    "        for t in range(self.skip_steps):\n",
    "            if self.TESTING:\n",
    "                self.draw_obj(agent_target, agent_null_torque)\n",
    "        \n",
    "            if self.render:\n",
    "                self.env.render()\n",
    "            \n",
    "            target = np.hstack([agent_target, np.array([0, 0, 0]),])\n",
    "            \n",
    "            action = self.controller.generate(q=self.q, dq=self.dq, target = target)\n",
    "            ob, reward, done, dict_ = self.env.step(action, ctrl_const = self.ctrl_const,\n",
    "                                                    crash_const = self.crash_const,\n",
    "                                                    sucess_const = self.final_success_const,\n",
    "                                                    time_const = self.time_const,\n",
    "                                                    finger_act = finger_act,\n",
    "                                                    fruit_grabbed =  self.aux_controller.fruit_grabbed,\n",
    "                                                    testing = self.TESTING)\n",
    "            step_reward += reward  \n",
    "            self.q = self.env.q\n",
    "            self.dq = self.env.dq\n",
    "            \n",
    "            if done and stop_early:\n",
    "                break\n",
    "        \n",
    "        self.dist_measure = self.env.distance_measure\n",
    "        self.energy_measure = self.env.energy_measure\n",
    "        self.collision = self.env.collision\n",
    "        self.finish_time = self.env.data.time\n",
    "        \n",
    "        return ob, step_reward, done, dict_\n",
    "    \n",
    "    def seed(self, num):\n",
    "        self.env.seed(num)\n",
    "    \n",
    "    def reset(self, area_perc = 1, repeat_perc = 0, custom_target_delta = None):\n",
    "        self.dist_measure = 0\n",
    "        self.energy_measure = 0\n",
    "        self.manipu_measure = 0\n",
    "        self.success_bool = False\n",
    "        self.env.area_perc = area_perc \n",
    "        self.env.repeat_perc = repeat_perc\n",
    "        self.env._max_episode_steps = self._max_episode_steps * self.skip_steps\n",
    "        \n",
    "        self.env.reset()\n",
    "        if np.any(custom_target_delta):\n",
    "            self.env.data.mocap_pos[1] = self.env.get_body_com(\"EE\") + custom_target_delta\n",
    "        \n",
    "        self.q = self.env.q\n",
    "        self.dq = self.env.dq.copy()\n",
    "        self.aux_controller.reset()\n",
    "        self.reset_step = True \n",
    "\n",
    "# Reward design parameter gains\n",
    "ctrl_const = -0.03 \n",
    "crash_const = -5 \n",
    "sucess_const = 2.2 \n",
    "time_const = -0.3\n",
    "\n",
    "# time = max_steps * skip_steps * dt\n",
    "max_steps = 20 \n",
    "skip_steps = 500\n",
    "aux_control = True\n",
    "\n",
    "agent_env = AgentEnv(env = env, skip_steps = skip_steps, controller = controller, aux_controller = null_controller,\n",
    "                     max_steps = max_steps, max_range = max_action_range.cpu().numpy(), ctrl_const = ctrl_const,\n",
    "                     crash_const = crash_const, sucess_const = sucess_const,\n",
    "                     time_const = time_const, aux_control = aux_control,\n",
    "                     danger_perc = danger_perc, warning_perc = warning_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07b5ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameter definition ---\n",
    "\n",
    "# Instanciacion del agente\n",
    "algorithm_name = \"SAC\"\n",
    "max_mem_lenght = 50000 \n",
    "max_score_mem = 100 \n",
    "max_stats_mem = 40\n",
    "batch_size = 256  \n",
    "epochs = 10000\n",
    "\n",
    "epsilon = 0.01 \n",
    "gamma = 0.99 \n",
    "actor_lr = 0.001\n",
    "q_critic_lr = 0.001\n",
    "tau = 0.005 \n",
    "actor_lr_interval = 0.5 \n",
    "critic_lr_interval = 0.25\n",
    "exp_epochs = 32\n",
    "\n",
    "agent = Agent(algorithm_name, max_mem_lenght, max_score_mem, max_stats_mem,\n",
    "              batch_size, max_steps, state_dim, action_dim, dist_thresh1, device, epsilon, gamma)\n",
    "\n",
    "target_alpha = -torch.Tensor((action_dim, )).to(device).item() \n",
    "log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "alpha_opt = torch.optim.Adam([log_alpha], lr=q_critic_lr)\n",
    "\n",
    "actor_opt = torch.optim.Adam(actor.parameters(), lr = actor_lr)\n",
    "q1_critic_opt = torch.optim.Adam(q1_critic.parameters(), lr = q_critic_lr)\n",
    "q2_critic_opt = torch.optim.Adam(q2_critic.parameters(), lr = q_critic_lr)\n",
    "\n",
    "rl_algorithm = SAC\n",
    "\n",
    "print_interval = max(int(epochs/1000), 1)\n",
    "save_interval =  max(int(epochs/10), 1)\n",
    "save_file_name = \"Actor\" \n",
    "\n",
    "alpha_max_epoch = int(epochs * 0.9) \n",
    "nets_list = [actor, q1_critic, q2_critic]\n",
    "targ_nets_list = [None, target_q1_critic, target_q2_critic]\n",
    "opt_list = [actor_opt, q1_critic_opt, q2_critic_opt, alpha_opt]\n",
    "agent_kwargs = {\"squashed\": True, \"stability_const\": 0.00001}\n",
    "repeat_max_perc = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5de36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Training start ---\n",
    "\n",
    "(stats_history,\n",
    " best_nets, final_nets) = train_agent(rl_algorithm = rl_algorithm, agent = agent, env = agent_env, device = device,\n",
    "                                      epochs = epochs, exp_epochs = exp_epochs, print_interval = print_interval,\n",
    "                                      save_interval = save_interval, save_file_name = save_file_name,\n",
    "                                      nets_list = nets_list, targ_nets_list = targ_nets_list, opt_list = opt_list,\n",
    "                                      actor_lr_interval = actor_lr_interval, critic_lr_interval = critic_lr_interval,\n",
    "                                      \n",
    "                                      gamma = gamma, agent_kwargs = agent_kwargs, tau = tau, target_alpha = target_alpha,\n",
    "                                      log_alpha = log_alpha, repeat_max_perc = repeat_max_perc)\n",
    "\n",
    "actor2 = Actor(\"SAC\", net_architecture(state_dim, hidden_dim), hidden_dim, action_dim, max_action_range,\n",
    "              min_std, max_std).to(device)\n",
    "\n",
    "actor2.load_state_dict(deepcopy(best_nets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(stats_history, stats_name, epochs, print_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8741629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Testing ---\n",
    "\n",
    "OSC_actor = DummyActor(env, action_dim = action_dim, max_range = max_action_range)\n",
    "\n",
    "env.fixed_ini = True\n",
    "agent_env.TESTING = True\n",
    "NEURAL_CONTROLLER = False\n",
    "\n",
    "test_steps = 30\n",
    "\n",
    "if NEURAL_CONTROLLER:\n",
    "    current_actor = actor2\n",
    "else:\n",
    "    current_actor = base_case_actor\n",
    "    \n",
    "glfw.init()\n",
    "agent_env._max_episode_steps = test_steps\n",
    "agent.reset(agent_env)\n",
    "test_steps -= 1 \n",
    "\n",
    "for t in range(test_steps):\n",
    "    \n",
    "    env.render()\n",
    "    mu, std, V, Q, total_reward, done = agent.test(agent_env, current_actor, device,\n",
    "                                                   critic = None, q_critic = None,\n",
    "                                                   stop_early = False)\n",
    "    \n",
    "    mani_array[t] = agent_env.manipulation_score\n",
    "    \n",
    "print(\"colision: \" + str(env.collision))\n",
    "print(\"success: \" + str(agent.goal_reached))\n",
    "print(\"total_reward: \" + str(total_reward))\n",
    "glfw.terminate() \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
